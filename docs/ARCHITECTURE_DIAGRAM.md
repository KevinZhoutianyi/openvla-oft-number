# OpenVLA Architecture & Training Flow

## ğŸ—ï¸ Model Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        OpenVLA Architecture                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Head Camera  â”‚  â”‚ Wrist Camera â”‚  â”‚ Language Instructionâ”‚
â”‚  (256Ã—256)   â”‚  â”‚  (256Ã—256)   â”‚  â”‚ "place the shoe"    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                 â”‚                      â”‚
       â”‚ Resize          â”‚ Resize               â”‚ Tokenize
       â†“                 â†“                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  (3Ã—224Ã—224) â”‚  â”‚  (3Ã—224Ã—224) â”‚  â”‚   input_ids         â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                 â”‚                      â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
                â†“                               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
        â”‚ Vision Encoderâ”‚ (SigLIP)              â”‚
        â”‚   + LoRA      â”‚ â† Trainable adapters  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
                â”‚                               â”‚
                â†“                               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
        â”‚  Projector    â”‚ â† Trainable (LoRA)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
                â”‚                               â”‚
                â†“                               â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚       Language Model (Llama/Vicuna)      â”‚
        â”‚            7B parameters                 â”‚
        â”‚          + LoRA adapters                 â”‚
        â”‚         â† Trainable adapters             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Hidden States   â”‚
                â”‚   (seq_len, 4096)â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Action Head    â”‚ â† Fully Trainable
                â”‚  L1 Regression   â”‚    (~50M params)
                â”‚  (4096 â†’ 14)     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â†“
OUTPUT:
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Action Chunk    â”‚
                â”‚   (8 Ã— 14)       â”‚
                â”‚                  â”‚
                â”‚ [timestep 0]: 14 â”‚ â† Current action
                â”‚ [timestep 1]: 14 â”‚
                â”‚ [timestep 2]: 14 â”‚
                â”‚     ...          â”‚
                â”‚ [timestep 7]: 14 â”‚ â† 7 steps ahead
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Training Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Single Training Step                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. LOAD FROM HDF5:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ episode_0.hdf5 [timestep t=10]  â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ head_camera_image[10]: (256,256,3) â”‚
   â”‚ left_wrist_image[10]:  (256,256,3) â”‚
   â”‚ action[10:18]:         (8, 14)     â”‚
   â”‚ seen: "place the shoe"             â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“

2. PREPROCESS:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ â€¢ Resize images to 224Ã—224      â”‚
   â”‚ â€¢ Normalize pixels to [-1, 1]   â”‚
   â”‚ â€¢ Tokenize language instruction â”‚
   â”‚ â€¢ Normalize actions             â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“

3. BATCH (size=8):
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ pixel_values:       (8, 3, 224, 224)â”‚
   â”‚ pixel_values_wrist: (8, 3, 224, 224)â”‚
   â”‚ input_ids:          (8, seq_len)    â”‚
   â”‚ labels:             (8, seq_len)    â”‚
   â”‚ actions:            (8, 8, 14)      â”‚ â† Ground truth
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“

4. FORWARD PASS:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ OpenVLA(images, text)              â”‚
   â”‚   â†’ vision_features                â”‚
   â”‚   â†’ text_features                  â”‚
   â”‚   â†’ fused_features                 â”‚
   â”‚   â†’ hidden_states (8, seq_len, 4096) â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ ActionHead(hidden_states)          â”‚
   â”‚   â†’ predicted_actions (8, 8, 14)   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“

5. COMPUTE LOSS:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ L1Loss(predicted_actions, gt_actions)  â”‚
   â”‚                                        â”‚
   â”‚ loss = |predicted - ground_truth|     â”‚
   â”‚                                        â”‚
   â”‚ Example:                               â”‚
   â”‚   predicted[0,0] = [0.12, -0.45, ...]  â”‚
   â”‚   ground_truth[0,0] = [0.13, -0.44, ...]â”‚
   â”‚   loss = mean(|0.12-0.13|, |-0.45-(-0.44)|, ...) â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“

6. BACKWARD PASS:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ loss.backward()                    â”‚
   â”‚   â†’ Compute gradients              â”‚
   â”‚   â†’ Update only:                   â”‚
   â”‚     â€¢ LoRA adapters (~14M params)  â”‚
   â”‚     â€¢ Action head (~50M params)    â”‚
   â”‚                                    â”‚
   â”‚   â†’ Base model stays frozen!       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“

7. UPDATE WEIGHTS:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ optimizer.step()                   â”‚
   â”‚   â†’ Apply gradients                â”‚
   â”‚   â†’ Learning rate: 5e-4            â”‚
   â”‚   â†’ AdamW optimizer                â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Action Chunking Visualization

```
Timeline:  t=10  t=11  t=12  t=13  t=14  t=15  t=16  t=17  t=18
           â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€>
Episode:   â”‚ â–“â–“ â”‚ â–“â–“ â”‚ â–“â–“ â”‚ â–“â–“ â”‚ â–“â–“ â”‚ â–“â–“ â”‚ â–“â–“ â”‚ â–“â–“ â”‚
           â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€

At timestep t=10, the model sees:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT:                                               â”‚
â”‚   â€¢ Image at t=10                                    â”‚
â”‚   â€¢ Language: "place the shoe"                       â”‚
â”‚                                                      â”‚
â”‚ OUTPUT (predict 8 future actions):                  â”‚
â”‚   action[0] = action to take NOW at t=10   â—„â”€ Execute this
â”‚   action[1] = action for t=11              â”‚
â”‚   action[2] = action for t=12              â”‚
â”‚   action[3] = action for t=13              â”œâ”€ Predict but
â”‚   action[4] = action for t=14              â”‚  don't use yet
â”‚   action[5] = action for t=15              â”‚
â”‚   action[6] = action for t=16              â”‚
â”‚   action[7] = action for t=17              â—„
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Why predict 8 steps?
  âœ“ Temporal consistency (smooth motions)
  âœ“ Reduces error accumulation
  âœ“ Better long-term planning
```

## ğŸ¯ Your Dataset Specifics

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Place Shoe Dataset Structure                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

EPISODES: 48 HDF5 files
â”œâ”€â”€ episode_0.hdf5
â”‚   â”œâ”€â”€ head_camera_image:   (185, 256, 256, 3) uint8
â”‚   â”œâ”€â”€ left_wrist_image:    (185, 256, 256, 3) uint8
â”‚   â”œâ”€â”€ right_wrist_image:   (185, 256, 256, 3) uint8
â”‚   â”œâ”€â”€ low_cam_image:       (185, 256, 256, 3) uint8
â”‚   â”œâ”€â”€ action:              (185, 14) float64
â”‚   â”œâ”€â”€ relative_action:     (185, 14) float64
â”‚   â”œâ”€â”€ seen:                "place the shoe in..."
â”‚   â””â”€â”€ unseen:              "put the shoe on..."
â”œâ”€â”€ episode_1.hdf5
â”‚   â””â”€â”€ ...
â””â”€â”€ episode_47.hdf5

TOTAL TRAINING SAMPLES:
  48 episodes Ã— ~178 timesteps per episode = ~8,544 samples
  (accounting for action chunking: 185 - 8 + 1 = 178)

ACTION SPACE (14 dimensions):
  [dim_0, dim_1, dim_2, ..., dim_13]
  â†‘                              â†‘
  Position/velocity          Gripper?
  (exact meaning depends on your robot)
```

## ğŸ”¢ Training Parameters

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Training Configuration                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

MODEL:
  Base: OpenVLA-7B (7 billion parameters)
  Trainable: ~64M parameters (LoRA + action head)
  
TRAINING:
  Batch size: 8 per GPU (Ã—4 GPUs = 32 total)
  Learning rate: 5e-4 â†’ 5e-5 (decay at 40k steps)
  Max steps: 50,000
  Optimizer: AdamW
  
DATA:
  Images: 2 (head + left wrist)
  Resolution: 224Ã—224
  Action dim: 14
  Chunk size: 8 timesteps
  
LOSS:
  Type: L1 (Mean Absolute Error)
  Target: ~0.01 or lower
  
CHECKPOINTS:
  Frequency: Every 5,000 steps
  Location: /work/hdd/bfdj/tzhou4/checkpoints/
```

## ğŸ“ˆ Expected Training Progress

```
Step      Loss    Curr L1   Future L1   Status
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0         1.500   1.200     1.600       Random init
100       0.450   0.350     0.500       Learning basics
1,000     0.120   0.090     0.140       Improving
5,000     0.035   0.025     0.042       Getting good
10,000    0.015   0.011     0.018       Almost there âœ“
20,000    0.010   0.007     0.012       Excellent âœ“âœ“
40,000    0.008   0.006     0.009       Great âœ“âœ“âœ“
50,000    0.007   0.005     0.008       Best âœ“âœ“âœ“âœ“

Target: Loss < 0.01 is good, < 0.008 is excellent
```

## ğŸ® Inference (After Training)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Using Trained Model                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

REAL-TIME DEPLOYMENT:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Robot at t=0   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Capture image
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenVLA-OFT            â”‚
â”‚  Input: image, "place  â”‚
â”‚         the shoe"      â”‚
â”‚  Output: [a0...a7]     â”‚ â† Predicted action chunk
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“ Execute action[0]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Robot moves    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ t=1
         â†“ Capture new image
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenVLA-OFT            â”‚
â”‚  Input: new image      â”‚
â”‚  Output: [a0'...a7']   â”‚ â† New predictions
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Execute action[0']
         â†“
       Continue...

Note: Can reuse actions from previous chunk
      or recompute every step (more robust)
```

---

## ğŸ“š Key Takeaways

1. **Input**: Images + Language â†’ **Output**: 8 future actions (14D each)

2. **Training**: Only ~64M params trainable (LoRA + action head), not 7B

3. **Loss**: L1 distance between predicted and ground truth actions

4. **Action Chunking**: Predicts 8 timesteps for temporal consistency

5. **Your Data**: 48 episodes, ~8.5k samples, 14D actions

For implementation details, see:
- `prismatic/vla/datasets/place_shoe_dataset.py`
- `vla-scripts/finetune_place_shoe.py`
- `prismatic/models/action_heads.py`

